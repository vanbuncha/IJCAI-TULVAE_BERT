{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HT6ZER3i_iAT"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.31.0 tqdm sklearn\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HoKNEjDbumLE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 1: Load the Check-in Data\n",
    "def load_checkins(file_path):\n",
    "    \"\"\"\n",
    "    Load Gowalla check-in data from a compressed file.\n",
    "    \"\"\"\n",
    "    with gzip.open(file_path, 'rt') as f:\n",
    "        # Column names based on Gowalla data description\n",
    "        columns = [\"user_id\", \"check_in_time\", \"latitude\", \"longitude\", \"location_id\"]\n",
    "        data = pd.read_csv(f, sep=\"\\t\", names=columns)\n",
    "\n",
    "    # Convert timestamp to datetime for easier manipulation\n",
    "    data[\"check_in_time\"] = pd.to_datetime(data[\"check_in_time\"])\n",
    "    return data\n",
    "\n",
    "checkins_file = \"loc-gowalla_totalCheckins.txt.gz\"\n",
    "gowalla_data = load_checkins(checkins_file)\n",
    "\n",
    "# Step 2: Construct User Trajectories\n",
    "def construct_trajectories(data, time_window=\"1D\"):\n",
    "    \"\"\"\n",
    "    Group check-ins into user trajectories based on a time window.\n",
    "    :param data: DataFrame of check-ins\n",
    "    :param time_window: Pandas time frequency string (e.g., '1D' for daily)\n",
    "    \"\"\"\n",
    "    trajectories = []\n",
    "\n",
    "    # Group by user and sort by timestamp\n",
    "    grouped = data.groupby(\"user_id\")\n",
    "    for user_id, group in grouped:\n",
    "        group = group.sort_values(\"check_in_time\")\n",
    "        group[\"trajectory_id\"] = (group[\"check_in_time\"].diff() > pd.Timedelta(time_window)).cumsum()\n",
    "\n",
    "        # Create a trajectory for each group\n",
    "        for traj_id, traj_group in group.groupby(\"trajectory_id\"):\n",
    "            trajectory = {\n",
    "                \"user_id\": user_id,\n",
    "                \"trajectory_id\": traj_id,\n",
    "                \"timestamps\": traj_group[\"check_in_time\"].tolist(),\n",
    "                \"locations\": traj_group[\"location_id\"].tolist(),\n",
    "            }\n",
    "            trajectories.append(trajectory)\n",
    "\n",
    "    return pd.DataFrame(trajectories)\n",
    "\n",
    "trajectories_df = construct_trajectories(gowalla_data)\n",
    "\n",
    "# Step 3: Encode POIs\n",
    "def encode_pois(trajectories):\n",
    "    \"\"\"\n",
    "    Encode POIs into unique numeric IDs.\n",
    "    :param trajectories: DataFrame containing trajectory information\n",
    "    \"\"\"\n",
    "    all_locations = set(loc for traj in trajectories[\"locations\"] for loc in traj)\n",
    "    location_mapping = {loc: idx for idx, loc in enumerate(all_locations)}\n",
    "\n",
    "    # Replace locations with their numeric IDs\n",
    "    trajectories[\"encoded_locations\"] = trajectories[\"locations\"].apply(\n",
    "        lambda locs: [location_mapping[loc] for loc in locs]\n",
    "    )\n",
    "    return trajectories, location_mapping\n",
    "\n",
    "trajectories_df, poi_mapping = encode_pois(trajectories_df)\n",
    "\n",
    "# Save\n",
    "trajectories_df.to_csv(\"gowalla_trajectories.csv\", index=False)\n",
    "\n",
    "# Display some trajectories\n",
    "print(trajectories_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDdcxiWozl8L",
    "outputId": "e852dc7a-7a31-4836-e0ee-99371ada45e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.31.0 tqdm sklearn\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNCCf_OEvJ4D",
    "outputId": "3210a864-dd43-43d4-8702-8d8f624477a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users (classes): 9066\n",
      "Label range: 0 to 9065\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Train Loss: 9.1221, Train Top-1 Acc: 0.0001, Train Top-5 Acc: 0.0004\n",
      "Val Loss: 9.1432, Val Top-1 Acc: 0.0000, Val Top-5 Acc: 0.0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Train Loss: 9.0192, Train Top-1 Acc: 0.0003, Train Top-5 Acc: 0.0019\n",
      "Val Loss: 9.5721, Val Top-1 Acc: 0.0000, Val Top-5 Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Train Loss: 8.9334, Train Top-1 Acc: 0.0004, Train Top-5 Acc: 0.0020\n",
      "Val Loss: 10.2561, Val Top-1 Acc: 0.0000, Val Top-5 Acc: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the Processed Data\n",
    "trajectories_df = pd.read_csv(\"gowalla_trajectories.csv\")\n",
    "\n",
    "# Use a smaller subset for faster preliminary experiments\n",
    "subset_size = 10000\n",
    "trajectories_df = trajectories_df.sample(n=subset_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "trajectories = trajectories_df[\"encoded_locations\"].apply(eval).tolist()\n",
    "labels = trajectories_df[\"user_id\"].tolist()\n",
    "\n",
    "# Step 2: Re-encode the Labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "num_users = len(label_encoder.classes_)\n",
    "print(f\"Number of users (classes): {num_users}\")\n",
    "print(f\"Label range: {min(labels)} to {max(labels)}\")\n",
    "\n",
    "# Train-test split\n",
    "train_trajectories, val_trajectories, train_labels, val_labels = train_test_split(\n",
    "    trajectories, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Define a Dataset Class for Trajectories\n",
    "class GowallaDataset(Dataset):\n",
    "    def __init__(self, trajectories, labels, tokenizer, max_length):\n",
    "        self.trajectories = trajectories\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        trajectory_str = \" \".join(map(str, trajectory))\n",
    "        inputs = self.tokenizer(\n",
    "            trajectory_str,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Step 4: Prepare Tokenizer and Datasets\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_length = 64\n",
    "\n",
    "train_dataset = GowallaDataset(train_trajectories, train_labels, tokenizer, max_length)\n",
    "val_dataset = GowallaDataset(val_trajectories, val_labels, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Step 5: Define the Model\n",
    "class TrajectoryDistilBERT(torch.nn.Module):\n",
    "    def __init__(self, num_users):\n",
    "        super(TrajectoryDistilBERT, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.dim, num_users)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # first token\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "model = TrajectoryDistilBERT(num_users)\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 6: Setup Optimizer and Loss\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Additional Metrics\n",
    "def compute_metrics(outputs, labels, top_k=5):\n",
    "    # outputs: (batch_size, num_classes)\n",
    "    # labels: (batch_size)\n",
    "    with torch.no_grad():\n",
    "        # Top-1 accuracy\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_top1 = (preds == labels).sum().item()\n",
    "        top1_acc = correct_top1 / labels.size(0)\n",
    "\n",
    "        # Top-k accuracy\n",
    "        topk_values, topk_indices = torch.topk(outputs, k=top_k, dim=1)\n",
    "        correct_topk = 0\n",
    "        for i in range(labels.size(0)):\n",
    "            if labels[i].item() in topk_indices[i]:\n",
    "                correct_topk += 1\n",
    "        topk_acc = correct_topk / labels.size(0)\n",
    "\n",
    "    return top1_acc, topk_acc\n",
    "\n",
    "def evaluate_model(model, loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_top1 = 0\n",
    "    total_top5 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            batch_size = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            top1_acc, top5_acc = compute_metrics(outputs, labels, top_k=5)\n",
    "            total_top1 += top1_acc * batch_size\n",
    "            total_top5 += top5_acc * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_top1 = total_top1 / total_samples\n",
    "    avg_top5 = total_top5 / total_samples\n",
    "    return avg_loss, avg_top1, avg_top5\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
    "\n",
    "        for batch in train_iter:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_iter.set_description(f\"Epoch {epoch+1} [Training] loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate on train set\n",
    "        train_loss, train_top1, train_top5 = evaluate_model(model, train_loader, device, loss_fn)\n",
    "        # Evaluate on validation set\n",
    "        val_loss, val_top1, val_top5 = evaluate_model(model, val_loader, device, loss_fn)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Top-1 Acc: {train_top1:.4f}, Train Top-5 Acc: {train_top5:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Top-1 Acc: {val_top1:.4f}, Val Top-5 Acc: {val_top5:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IEjF7gmi6xav"
   },
   "source": [
    "### Lowering number of classes to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M-20lvuE6w-t",
    "outputId": "76a9387f-f764-4928-af3a-7f653d5c047d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.31.0\n",
      "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
      "\u001b[1;36mhint\u001b[0m: See above for details.\n",
      "Number of users (classes): 1000\n",
      "Label range: 0 to 999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1:\n",
      "Train Loss: 5.9143, Train Top-1 Acc: 0.1454, Train Top-5 Acc: 0.2515\n",
      "Val Loss: 6.0755, Val Top-1 Acc: 0.1248, Val Top-5 Acc: 0.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2:\n",
      "Train Loss: 4.9110, Train Top-1 Acc: 0.2624, Train Top-5 Acc: 0.3943\n",
      "Val Loss: 5.2770, Val Top-1 Acc: 0.2217, Val Top-5 Acc: 0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3:\n",
      "Train Loss: 4.1185, Train Top-1 Acc: 0.3573, Train Top-5 Acc: 0.5034\n",
      "Val Loss: 4.6998, Val Top-1 Acc: 0.2954, Val Top-5 Acc: 0.3984\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load and Filter the Data to Reduce Number of Classes\n",
    "\n",
    "# Load the dataset\n",
    "trajectories_df = pd.read_csv(\"gowalla_trajectories.csv\")\n",
    "\n",
    "# Count occurrences per user\n",
    "user_counts = Counter(trajectories_df['user_id'])\n",
    "\n",
    "# Choose top N users (adjust N as desired)\n",
    "N = 1000\n",
    "top_users = {user for user, count in user_counts.most_common(N)}\n",
    "\n",
    "# Filter the DataFrame to only keep trajectories of top N users\n",
    "filtered_df = trajectories_df[trajectories_df['user_id'].isin(top_users)].reset_index(drop=True)\n",
    "\n",
    "# Prepare Data: Label Encoding and Train/Test Split\n",
    "\n",
    "trajectories = filtered_df[\"encoded_locations\"].apply(eval).tolist()\n",
    "labels = filtered_df[\"user_id\"].tolist()\n",
    "\n",
    "# Label encode the users\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "num_users = len(label_encoder.classes_)\n",
    "print(f\"Number of users (classes): {num_users}\")\n",
    "print(f\"Label range: {min(labels)} to {max(labels)}\")\n",
    "\n",
    "train_trajectories, val_trajectories, train_labels, val_labels = train_test_split(\n",
    "    trajectories, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 2: Define the Dataset Class\n",
    "\n",
    "class GowallaDataset(Dataset):\n",
    "    def __init__(self, trajectories, labels, tokenizer, max_length):\n",
    "        self.trajectories = trajectories\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        trajectory = self.trajectories[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Convert the trajectory (list of POIs) to a string\n",
    "        trajectory_str = \" \".join(map(str, trajectory))\n",
    "\n",
    "        # Tokenize input sequence\n",
    "        inputs = self.tokenizer(\n",
    "            trajectory_str,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Step 3: Create Datasets and Loaders\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "max_length = 64\n",
    "\n",
    "train_dataset = GowallaDataset(train_trajectories, train_labels, tokenizer, max_length)\n",
    "val_dataset = GowallaDataset(val_trajectories, val_labels, tokenizer, max_length)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "# Step 4: Define the Model\n",
    "\n",
    "class TrajectoryDistilBERT(torch.nn.Module):\n",
    "    def __init__(self, num_users):\n",
    "        super(TrajectoryDistilBERT, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.classifier = torch.nn.Linear(self.bert.config.dim, num_users)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # DistilBERT: last_hidden_state is (batch_size, seq_len, hidden_size)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "\n",
    "model = TrajectoryDistilBERT(num_users)\n",
    "\n",
    "# Detect device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Step 5: Setup Optimizer, Loss, and Metrics\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def compute_metrics(outputs, labels, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_top1 = (preds == labels).sum().item()\n",
    "        top1_acc = correct_top1 / labels.size(0)\n",
    "\n",
    "        topk_values, topk_indices = torch.topk(outputs, k=top_k, dim=1)\n",
    "        correct_topk = 0\n",
    "        for i in range(labels.size(0)):\n",
    "            if labels[i].item() in topk_indices[i]:\n",
    "                correct_topk += 1\n",
    "        topk_acc = correct_topk / labels.size(0)\n",
    "\n",
    "    return top1_acc, topk_acc\n",
    "\n",
    "def evaluate_model(model, loader, device, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_top1 = 0\n",
    "    total_top5 = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            batch_size = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            top1_acc, top5_acc = compute_metrics(outputs, labels, top_k=5)\n",
    "            total_top1 += top1_acc * batch_size\n",
    "            total_top5 += top5_acc * batch_size\n",
    "            total_samples += batch_size\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_top1 = total_top1 / total_samples\n",
    "    avg_top5 = total_top5 / total_samples\n",
    "    return avg_loss, avg_top1, avg_top5\n",
    "\n",
    "# Step 6: Training Loop\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
    "\n",
    "        for batch in train_iter:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            train_iter.set_description(f\"Epoch {epoch+1} [Training] loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Evaluate on train and val sets\n",
    "        train_loss, train_top1, train_top5 = evaluate_model(model, train_loader, device, loss_fn)\n",
    "        val_loss, val_top1, val_top5 = evaluate_model(model, val_loader, device, loss_fn)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Top-1 Acc: {train_top1:.4f}, Train Top-5 Acc: {train_top5:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Top-1 Acc: {val_top1:.4f}, Val Top-5 Acc: {val_top5:.4f}\")\n",
    "\n",
    "# Run training\n",
    "train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
