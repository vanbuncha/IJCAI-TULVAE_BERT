{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HT6ZER3i_iAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3c4629-3662-4035-dd19-aa2cdfc32137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.31.0 tqdm sklearn\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "import gzip\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HoKNEjDbumLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2508ca3f-18fb-443f-dc9d-370a30488ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   user_id  trajectory_id                                         timestamps  \\\n",
            "0        0              0  [2010-05-22 02:49:04+00:00, 2010-05-22 17:50:5...   \n",
            "1        0              1                        [2010-05-26 14:16:56+00:00]   \n",
            "2        0              2                        [2010-05-27 22:39:52+00:00]   \n",
            "3        0              3                        [2010-05-30 06:17:57+00:00]   \n",
            "4        0              4  [2010-05-31 20:10:37+00:00, 2010-06-01 14:38:2...   \n",
            "\n",
            "                                           locations  \\\n",
            "0  [608105, 8977, 18574, 17269, 1161876, 1163401,...   \n",
            "1                                            [21714]   \n",
            "2                                           [420315]   \n",
            "3                                             [9073]   \n",
            "4                     [18417, 480992, 15326, 420315]   \n",
            "\n",
            "                                   encoded_locations  \n",
            "0  [476320, 517, 8840, 7664, 860424, 861436, 520165]  \n",
            "1                                            [11347]  \n",
            "2                                           [336123]  \n",
            "3                                              [562]  \n",
            "4                       [8694, 382410, 5990, 336123]  \n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the Check-in Data\n",
        "def load_checkins(file_path):\n",
        "    \"\"\"\n",
        "    Load Gowalla check-in data from a compressed file.\n",
        "    \"\"\"\n",
        "    with gzip.open(file_path, 'rt') as f:\n",
        "        # Column names based on Gowalla data description\n",
        "        columns = [\"user_id\", \"check_in_time\", \"latitude\", \"longitude\", \"location_id\"]\n",
        "        data = pd.read_csv(f, sep=\"\\t\", names=columns)\n",
        "\n",
        "    # Convert timestamp to datetime for easier manipulation\n",
        "    data[\"check_in_time\"] = pd.to_datetime(data[\"check_in_time\"])\n",
        "    return data\n",
        "\n",
        "checkins_file = \"loc-gowalla_totalCheckins.txt.gz\"\n",
        "gowalla_data = load_checkins(checkins_file)\n",
        "\n",
        "# Step 2: Construct User Trajectories\n",
        "def construct_trajectories(data, time_window=\"1D\"):\n",
        "    \"\"\"\n",
        "    Group check-ins into user trajectories based on a time window.\n",
        "    :param data: DataFrame of check-ins\n",
        "    :param time_window: Pandas time frequency string (e.g., '1D' for daily)\n",
        "    \"\"\"\n",
        "    trajectories = []\n",
        "\n",
        "    # Group by user and sort by timestamp\n",
        "    grouped = data.groupby(\"user_id\")\n",
        "    for user_id, group in grouped:\n",
        "        group = group.sort_values(\"check_in_time\")\n",
        "        group[\"trajectory_id\"] = (group[\"check_in_time\"].diff() > pd.Timedelta(time_window)).cumsum()\n",
        "\n",
        "        # Create a trajectory for each group\n",
        "        for traj_id, traj_group in group.groupby(\"trajectory_id\"):\n",
        "            trajectory = {\n",
        "                \"user_id\": user_id,\n",
        "                \"trajectory_id\": traj_id,\n",
        "                \"timestamps\": traj_group[\"check_in_time\"].tolist(),\n",
        "                \"locations\": traj_group[\"location_id\"].tolist(),\n",
        "            }\n",
        "            trajectories.append(trajectory)\n",
        "\n",
        "    return pd.DataFrame(trajectories)\n",
        "\n",
        "trajectories_df = construct_trajectories(gowalla_data)\n",
        "\n",
        "# Step 3: Encode POIs\n",
        "def encode_pois(trajectories):\n",
        "    \"\"\"\n",
        "    Encode POIs into unique numeric IDs.\n",
        "    :param trajectories: DataFrame containing trajectory information\n",
        "    \"\"\"\n",
        "    all_locations = set(loc for traj in trajectories[\"locations\"] for loc in traj)\n",
        "    location_mapping = {loc: idx for idx, loc in enumerate(all_locations)}\n",
        "\n",
        "    # Replace locations with their numeric IDs\n",
        "    trajectories[\"encoded_locations\"] = trajectories[\"locations\"].apply(\n",
        "        lambda locs: [location_mapping[loc] for loc in locs]\n",
        "    )\n",
        "    return trajectories, location_mapping\n",
        "\n",
        "trajectories_df, poi_mapping = encode_pois(trajectories_df)\n",
        "\n",
        "# Save\n",
        "trajectories_df.to_csv(\"gowalla_trajectories.csv\", index=False)\n",
        "\n",
        "# Display some trajectories\n",
        "print(trajectories_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDdcxiWozl8L",
        "outputId": "e852dc7a-7a31-4836-e0ee-99371ada45e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Using cached transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.31.0 tqdm sklearn\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "BNCCf_OEvJ4D",
        "outputId": "9d72a3e7-427c-4797-8ec9-ba11f31f62e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users (classes): 9066\n",
            "Label range: 0 to 9065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-98288100985a>\u001b[0m in \u001b[0;36m<cell line: 169>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-98288100985a>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, device, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mtrain_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1} [Training] loss: {loss.item():.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Step 1: Load the Processed Data\n",
        "trajectories_df = pd.read_csv(\"gowalla_trajectories.csv\")\n",
        "\n",
        "# Use a smaller subset for faster preliminary experiments\n",
        "subset_size = 10000\n",
        "trajectories_df = trajectories_df.sample(n=subset_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "trajectories = trajectories_df[\"encoded_locations\"].apply(eval).tolist()\n",
        "labels = trajectories_df[\"user_id\"].tolist()\n",
        "\n",
        "# Step 2: Re-encode the Labels\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "num_users = len(label_encoder.classes_)\n",
        "print(f\"Number of users (classes): {num_users}\")\n",
        "print(f\"Label range: {min(labels)} to {max(labels)}\")\n",
        "\n",
        "# Train-test split\n",
        "train_trajectories, val_trajectories, train_labels, val_labels = train_test_split(\n",
        "    trajectories, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define a Dataset Class for Trajectories\n",
        "class GowallaDataset(Dataset):\n",
        "    def __init__(self, trajectories, labels, tokenizer, max_length):\n",
        "        self.trajectories = trajectories\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        trajectory = self.trajectories[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        trajectory_str = \" \".join(map(str, trajectory))\n",
        "        inputs = self.tokenizer(\n",
        "            trajectory_str,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Step 4: Prepare Tokenizer and Datasets\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_length = 64\n",
        "\n",
        "train_dataset = GowallaDataset(train_trajectories, train_labels, tokenizer, max_length)\n",
        "val_dataset = GowallaDataset(val_trajectories, val_labels, tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Step 5: Define the Model\n",
        "class TrajectoryDistilBERT(torch.nn.Module):\n",
        "    def __init__(self, num_users):\n",
        "        super(TrajectoryDistilBERT, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.classifier = torch.nn.Linear(self.bert.config.dim, num_users)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # first token\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "model = TrajectoryDistilBERT(num_users)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 6: Setup Optimizer and Loss\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Additional Metrics\n",
        "def compute_metrics(outputs, labels, top_k=5):\n",
        "    # outputs: (batch_size, num_classes)\n",
        "    # labels: (batch_size)\n",
        "    with torch.no_grad():\n",
        "        # Top-1 accuracy\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct_top1 = (preds == labels).sum().item()\n",
        "        top1_acc = correct_top1 / labels.size(0)\n",
        "\n",
        "        # Top-k accuracy\n",
        "        topk_values, topk_indices = torch.topk(outputs, k=top_k, dim=1)\n",
        "        correct_topk = 0\n",
        "        for i in range(labels.size(0)):\n",
        "            if labels[i].item() in topk_indices[i]:\n",
        "                correct_topk += 1\n",
        "        topk_acc = correct_topk / labels.size(0)\n",
        "\n",
        "    return top1_acc, topk_acc\n",
        "\n",
        "def evaluate_model(model, loader, device, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_top1 = 0\n",
        "    total_top5 = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            batch_size = labels.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "\n",
        "            top1_acc, top5_acc = compute_metrics(outputs, labels, top_k=5)\n",
        "            total_top1 += top1_acc * batch_size\n",
        "            total_top5 += top5_acc * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_top1 = total_top1 / total_samples\n",
        "    avg_top5 = total_top5 / total_samples\n",
        "    return avg_loss, avg_top1, avg_top5\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
        "\n",
        "        for batch in train_iter:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            train_iter.set_description(f\"Epoch {epoch+1} [Training] loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Evaluate on train set\n",
        "        train_loss, train_top1, train_top5 = evaluate_model(model, train_loader, device, loss_fn)\n",
        "        # Evaluate on validation set\n",
        "        val_loss, val_top1, val_top5 = evaluate_model(model, val_loader, device, loss_fn)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}:\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Top-1 Acc: {train_top1:.4f}, Train Top-5 Acc: {train_top5:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Top-1 Acc: {val_top1:.4f}, Val Top-5 Acc: {val_top5:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEjF7gmi6xav"
      },
      "source": [
        "### Lowering number of classes to 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-20lvuE6w-t",
        "outputId": "1bef55f9-b3e7-4551-950d-408cbcfb245b"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of users (classes): 1000\n",
            "Label range: 0 to 999\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1:\n",
            "Train Loss: 5.8945, Train Top-1 Acc: 0.1408, Train Top-5 Acc: 0.2451\n",
            "Val Loss: 6.0577, Val Top-1 Acc: 0.1206, Val Top-5 Acc: 0.2111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2:\n",
            "Train Loss: 4.8986, Train Top-1 Acc: 0.2624, Train Top-5 Acc: 0.3955\n",
            "Val Loss: 5.2667, Val Top-1 Acc: 0.2262, Val Top-5 Acc: 0.3313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3:\n",
            "Train Loss: 4.1054, Train Top-1 Acc: 0.3522, Train Top-5 Acc: 0.4997\n",
            "Val Loss: 4.7002, Val Top-1 Acc: 0.2934, Val Top-5 Acc: 0.4002\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Load and Filter the Data to Reduce Number of Classes\n",
        "\n",
        "# Load the dataset\n",
        "trajectories_df = pd.read_csv(\"gowalla_trajectories.csv\")\n",
        "\n",
        "# Count occurrences per user\n",
        "user_counts = Counter(trajectories_df['user_id'])\n",
        "\n",
        "# Choose top N users (adjust N as desired)\n",
        "N = 1000\n",
        "top_users = {user for user, count in user_counts.most_common(N)}\n",
        "\n",
        "# Filter the DataFrame to only keep trajectories of top N users\n",
        "filtered_df = trajectories_df[trajectories_df['user_id'].isin(top_users)].reset_index(drop=True)\n",
        "\n",
        "# Prepare Data: Label Encoding and Train/Test Split\n",
        "\n",
        "trajectories = filtered_df[\"encoded_locations\"].apply(eval).tolist()\n",
        "labels = filtered_df[\"user_id\"].tolist()\n",
        "\n",
        "# Label encode the users\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "num_users = len(label_encoder.classes_)\n",
        "print(f\"Number of users (classes): {num_users}\")\n",
        "print(f\"Label range: {min(labels)} to {max(labels)}\")\n",
        "\n",
        "train_trajectories, val_trajectories, train_labels, val_labels = train_test_split(\n",
        "    trajectories, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Define the Dataset Class\n",
        "\n",
        "class GowallaDataset(Dataset):\n",
        "    def __init__(self, trajectories, labels, tokenizer, max_length):\n",
        "        self.trajectories = trajectories\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        trajectory = self.trajectories[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert the trajectory (list of POIs) to a string\n",
        "        trajectory_str = \" \".join(map(str, trajectory))\n",
        "\n",
        "        # Tokenize input sequence\n",
        "        inputs = self.tokenizer(\n",
        "            trajectory_str,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Step 3: Create Datasets and Loaders\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_length = 64\n",
        "\n",
        "train_dataset = GowallaDataset(train_trajectories, train_labels, tokenizer, max_length)\n",
        "val_dataset = GowallaDataset(val_trajectories, val_labels, tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Step 4: Define the Model\n",
        "\n",
        "class TrajectoryDistilBERT(torch.nn.Module):\n",
        "    def __init__(self, num_users):\n",
        "        super(TrajectoryDistilBERT, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.classifier = torch.nn.Linear(self.bert.config.dim, num_users)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DistilBERT: last_hidden_state is (batch_size, seq_len, hidden_size)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "model = TrajectoryDistilBERT(num_users)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 5: Setup Optimizer, Loss, and Metrics\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_metrics(outputs, labels, top_k=5):\n",
        "    with torch.no_grad():\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        correct_top1 = (preds == labels).sum().item()\n",
        "        top1_acc = correct_top1 / labels.size(0)\n",
        "\n",
        "        topk_values, topk_indices = torch.topk(outputs, k=top_k, dim=1)\n",
        "        correct_topk = 0\n",
        "        for i in range(labels.size(0)):\n",
        "            if labels[i].item() in topk_indices[i]:\n",
        "                correct_topk += 1\n",
        "        topk_acc = correct_topk / labels.size(0)\n",
        "\n",
        "    return top1_acc, topk_acc\n",
        "\n",
        "def evaluate_model(model, loader, device, loss_fn):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_top1 = 0\n",
        "    total_top5 = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            batch_size = labels.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "\n",
        "            top1_acc, top5_acc = compute_metrics(outputs, labels, top_k=5)\n",
        "            total_top1 += top1_acc * batch_size\n",
        "            total_top5 += top5_acc * batch_size\n",
        "            total_samples += batch_size\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    avg_top1 = total_top1 / total_samples\n",
        "    avg_top5 = total_top5 / total_samples\n",
        "    return avg_loss, avg_top1, avg_top5\n",
        "\n",
        "# Step 6: Training Loop\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
        "\n",
        "        for batch in train_iter:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            train_iter.set_description(f\"Epoch {epoch+1} [Training] loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Evaluate on train and val sets\n",
        "        train_loss, train_top1, train_top5 = evaluate_model(model, train_loader, device, loss_fn)\n",
        "        val_loss, val_top1, val_top5 = evaluate_model(model, val_loader, device, loss_fn)\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}:\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Top-1 Acc: {train_top1:.4f}, Train Top-5 Acc: {train_top5:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Top-1 Acc: {val_top1:.4f}, Val Top-5 Acc: {val_top5:.4f}\")\n",
        "\n",
        "# Run training\n",
        "train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and Filter the Data\n",
        "trajectories_df = pd.read_csv(\"gowalla_trajectories.csv\")\n",
        "\n",
        "# Count occurrences per user\n",
        "user_counts = Counter(trajectories_df['user_id'])\n",
        "\n",
        "# Choose top N users (adjust N as desired)\n",
        "N = 1000\n",
        "top_users = {user for user, count in user_counts.most_common(N)}\n",
        "\n",
        "# Filter the DataFrame to only keep trajectories of top N users\n",
        "filtered_df = trajectories_df[trajectories_df['user_id'].isin(top_users)].reset_index(drop=True)\n",
        "\n",
        "# Prepare Data: Label Encoding and Train/Test Split\n",
        "trajectories = filtered_df[\"encoded_locations\"].apply(eval).tolist()\n",
        "labels = filtered_df[\"user_id\"].tolist()\n",
        "\n",
        "# Label encode the users\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "num_users = len(label_encoder.classes_)\n",
        "print(f\"Number of users (classes): {num_users}\")\n",
        "print(f\"Label range: {min(labels)} to {max(labels)}\")\n",
        "\n",
        "train_trajectories, val_trajectories, train_labels, val_labels = train_test_split(\n",
        "    trajectories, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Define the Dataset Class\n",
        "class GowallaDataset(Dataset):\n",
        "    def __init__(self, trajectories, labels, tokenizer, max_length):\n",
        "        self.trajectories = trajectories\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.trajectories)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        trajectory = self.trajectories[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Convert the trajectory (list of POIs) to a string\n",
        "        trajectory_str = \" \".join(map(str, trajectory))\n",
        "\n",
        "        # Tokenize input sequence\n",
        "        inputs = self.tokenizer(\n",
        "            trajectory_str,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Step 3: Create Datasets and Loaders\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "max_length = 64\n",
        "\n",
        "train_dataset = GowallaDataset(train_trajectories, train_labels, tokenizer, max_length)\n",
        "val_dataset = GowallaDataset(val_trajectories, val_labels, tokenizer, max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "# Step 4: Define the Model\n",
        "class TrajectoryDistilBERT(nn.Module):\n",
        "    def __init__(self, num_users):\n",
        "        super(TrajectoryDistilBERT, self).__init__()\n",
        "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.classifier = nn.Linear(self.bert.config.dim, num_users)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # DistilBERT: last_hidden_state is (batch_size, seq_len, hidden_size)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "model = TrajectoryDistilBERT(num_users)\n",
        "\n",
        "# Detect device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Step 5: Setup Optimizer, Loss, and Metrics\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def compute_metrics(outputs, labels, top_ks=[1, 5]):\n",
        "    \"\"\"\n",
        "    Compute ACC@K for given outputs and labels.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    with torch.no_grad():\n",
        "        for k in top_ks:\n",
        "            _, topk_indices = torch.topk(outputs, k=k, dim=1)\n",
        "            correct_topk = 0\n",
        "            for i in range(labels.size(0)):\n",
        "                if labels[i].item() in topk_indices[i]:\n",
        "                    correct_topk += 1\n",
        "            metrics[f\"ACC@{k}\"] = correct_topk / labels.size(0)\n",
        "\n",
        "    # Compute Top-1 Accuracy for F1\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    metrics[\"top1_preds\"] = preds\n",
        "    return metrics\n",
        "\n",
        "def evaluate_model(model, loader, device, loss_fn):\n",
        "    \"\"\"\n",
        "    Evaluate the model and compute metrics like ACC@1, ACC@5, and F1 scores.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    acc_metrics = {\"ACC@1\": 0, \"ACC@5\": 0}\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            batch_size = labels.size(0)\n",
        "            total_loss += loss.item() * batch_size\n",
        "\n",
        "            # Compute ACC@K\n",
        "            batch_metrics = compute_metrics(outputs, labels)\n",
        "            acc_metrics[\"ACC@1\"] += batch_metrics[\"ACC@1\"] * batch_size\n",
        "            acc_metrics[\"ACC@5\"] += batch_metrics[\"ACC@5\"] * batch_size\n",
        "\n",
        "            # Collect predictions for F1 computation\n",
        "            all_preds.extend(batch_metrics[\"top1_preds\"].cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            total_samples += batch_size\n",
        "\n",
        "    # Compute average loss and accuracy\n",
        "    avg_loss = total_loss / total_samples\n",
        "    acc_metrics[\"ACC@1\"] /= total_samples\n",
        "    acc_metrics[\"ACC@5\"] /= total_samples\n",
        "\n",
        "    # Generate F1 scores\n",
        "    classification_metrics = classification_report(all_labels, all_preds, output_dict=True)\n",
        "\n",
        "    return avg_loss, acc_metrics, classification_metrics\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=3):\n",
        "    \"\"\"\n",
        "    Training loop for the model with metrics.\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\", leave=False)\n",
        "\n",
        "        for batch in train_iter:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            train_iter.set_description(f\"Epoch {epoch+1} [Training] loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Evaluate on train and validation sets\n",
        "        train_loss, train_acc_metrics, train_classification_metrics = evaluate_model(\n",
        "            model, train_loader, device, loss_fn\n",
        "        )\n",
        "        val_loss, val_acc_metrics, val_classification_metrics = evaluate_model(\n",
        "            model, val_loader, device, loss_fn\n",
        "        )\n",
        "\n",
        "        # Print Metrics\n",
        "        print(f\"\\nEpoch {epoch+1}:\")\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train ACC@1: {train_acc_metrics['ACC@1']:.4f}, Train ACC@5: {train_acc_metrics['ACC@5']:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val ACC@1: {val_acc_metrics['ACC@1']:.4f}, Val ACC@5: {val_acc_metrics['ACC@5']:.4f}\")\n",
        "        print(f\"Validation Macro F1: {val_classification_metrics['macro avg']['f1-score']:.4f}, Weighted F1: {val_classification_metrics['weighted avg']['f1-score']:.4f}\")\n",
        "\n",
        "# Run training\n",
        "train_model(model, train_loader, val_loader, device, loss_fn, optimizer, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXd2Vd7wj3nE",
        "outputId": "54061bd0-80b5-4fff-d056-96aa8fe0aaec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users (classes): 1000\n",
            "Label range: 0 to 999\n",
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1:\n",
            "Train Loss: 5.9769, Train ACC@1: 0.1302, Train ACC@5: 0.2216\n",
            "Val Loss: 6.1263, Val ACC@1: 0.1095, Val ACC@5: 0.1857\n",
            "Validation Macro F1: 0.0770, Weighted F1: 0.0782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2:\n",
            "Train Loss: 4.9922, Train ACC@1: 0.2549, Train ACC@5: 0.3783\n",
            "Val Loss: 5.3365, Val ACC@1: 0.2212, Val ACC@5: 0.3168\n",
            "Validation Macro F1: 0.1787, Weighted F1: 0.1824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3:\n",
            "Train Loss: 4.1828, Train ACC@1: 0.3401, Train ACC@5: 0.4830\n",
            "Val Loss: 4.7523, Val ACC@1: 0.2837, Val ACC@5: 0.3867\n",
            "Validation Macro F1: 0.2428, Weighted F1: 0.2482\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4:\n",
            "Train Loss: 3.5063, Train ACC@1: 0.4281, Train ACC@5: 0.5848\n",
            "Val Loss: 4.3175, Val ACC@1: 0.3416, Val ACC@5: 0.4440\n",
            "Validation Macro F1: 0.3099, Weighted F1: 0.3162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5:\n",
            "Train Loss: 2.9177, Train ACC@1: 0.5063, Train ACC@5: 0.6746\n",
            "Val Loss: 3.9978, Val ACC@1: 0.3836, Val ACC@5: 0.4850\n",
            "Validation Macro F1: 0.3572, Weighted F1: 0.3641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}